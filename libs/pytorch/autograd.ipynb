{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595487970483",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자동 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\n"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[27., 27.],\n        [27., 27.]], grad_fn=<MulBackward0>)\n"
    }
   ],
   "source": [
    "z = 3*(x+2)**2\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[18.0000, 18.0000],\n        [18.0000,  1.8000]])\n"
    }
   ],
   "source": [
    "# x.backward() == x.backward(torch.tensor(1))\n",
    "z.backward(torch.tensor([[1, 1], [1, 0.1]]))\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(2., grad_fn=<NormBackward0>)\n"
    }
   ],
   "source": [
    "print(x.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # don't track\n",
    "    y1 = x+3\n",
    "y2 = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "False\nTrue\n"
    }
   ],
   "source": [
    "print(y1.requires_grad)\n",
    "print(y2.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "False\ntensor([[True, True],\n        [True, True]])\n"
    }
   ],
   "source": [
    "x1 = x.detach()\n",
    "\n",
    "print(x1.requires_grad)\n",
    "print(x1.eq(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define new autograd function\n",
    "\n",
    "* static method로 forward와 backward함수를 구현해야 함\n",
    "* `forward(ctx, tensor_i) -> tensor_o`\n",
    "    * ctx(context object) backward에서 연산을 위해 필요한 값을 저장하는 변수\n",
    "        * ctx.save_for_backward(tensor)\n",
    "    * 연산된 tensor를 반환\n",
    "* `backward(ctx, grad_o) -> grad_i`\n",
    "    * 출력의 손실 변화도와 저장된 값을 이용해 입력 텐서의 손실 변화도를 계산해 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor_i):\n",
    "        ctx.save_for_backward(tensor_i)\n",
    "        return tensor_i.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_o):\n",
    "        tensor_i, = ctx.saved_tensors\n",
    "        grad_i = grad_o.clone()\n",
    "        grad_i[tensor_i < 0] = 0\n",
    "        return grad_i"
   ]
  }
 ]
}